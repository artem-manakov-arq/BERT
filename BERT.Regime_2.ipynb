{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zFxy9VfywWj0",
    "outputId": "0548d195-728c-4c4d-a575-6706aa50f241"
   },
   "outputs": [],
   "source": [
    "# https://github.com/google-research/bert\n",
    "# https://github.com/CyberZHG/keras-bert\n",
    "\n",
    "# folder name for BERT storing\n",
    "folder = 'multi_cased_L-12_H-768_A-12'\n",
    "# link to download pre-trained neural network BERT\n",
    "download_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip' \n",
    "\n",
    "print('Downloading model...')\n",
    "zip_path = '{}.zip'.format(folder)\n",
    "!test -d $folder || (wget $download_url && unzip $zip_path)\n",
    "\n",
    "# dowload tokenization.py from BERT repository\n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
    "\n",
    "# install Keras BERT\n",
    "!pip install keras-bert\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "import tokenization\n",
    "\n",
    "config_path = folder+'/bert_config.json'\n",
    "checkpoint_path = folder+'/bert_model.ckpt'\n",
    "vocab_path = folder+'/vocab.txt'\n",
    "\n",
    "# create an object for transforming sentence with spaces into tokens \n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
    "\n",
    "# Loading the model\n",
    "print('Loading model...')\n",
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n",
    "#model.summary()          # neural network parameters\n",
    "print('OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FjXLJupvwoFx",
    "outputId": "b053b758-8f31-495e-8939-d34169dffe30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In need of some fencing to hold a pig in and food got all burnt out through fire anyone able to help please?  -> I have fence material.\n",
      "Sentences is matched on: 99.29 %\n"
     ]
    }
   ],
   "source": [
    "# BERT Regime #2: What is probaility that 2nd sentence can be logical continue of the 1st sentence.\n",
    "sentence_1 = 'In need of some fencing to hold a pig in and food got all burnt out through fire anyone able to help please? '      #@param {type:\"string\"}\n",
    "sentence_2 = 'I have fence material.'          #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "print(sentence_1, '->', sentence_2)\n",
    "\n",
    "# Make tokenize of the sentences\n",
    "tokens_sen_1 = tokenizer.tokenize(sentence_1)\n",
    "tokens_sen_2 = tokenizer.tokenize(sentence_2)\n",
    "\n",
    "tokens = ['[CLS]'] + tokens_sen_1 + ['[SEP]'] + tokens_sen_2 + ['[SEP]']\n",
    "#print(tokens)\n",
    "\n",
    "# tranform string tokens into integer indexes:\n",
    "token_input = tokenizer.convert_tokens_to_ids(tokens)  \n",
    "# make the length 512      \n",
    "token_input = token_input + [0] * (512 - len(token_input))\n",
    "\n",
    "# the mask consist of all zeros\n",
    "mask_input = [0] * 512\n",
    "\n",
    "# in the mask of the 2nd sentence, including final SEP, we need to put 1, and all others is 0\n",
    "seg_input = [0]*512\n",
    "len_1 = len(tokens_sen_1) + 2                   # 1st phrase length , +2 - including original CLS Ð¸ delilmenter SEP\n",
    "for i in range(len(tokens_sen_2)+1):            # +1, including last SEP\n",
    "        seg_input[len_1 + i] = 1                # make mask for 2nd phrase, including last SEP, by 1\n",
    "#print(seg_input)\n",
    "\n",
    "\n",
    "# convert into pythin array  (1,) -> (1,512)\n",
    "token_input = np.asarray([token_input])\n",
    "mask_input = np.asarray([mask_input])\n",
    "seg_input = np.asarray([seg_input])\n",
    "\n",
    "\n",
    "# process sentences through the neural network...\n",
    "predicts = model.predict([token_input, seg_input, mask_input])[1]       #  in [1] answer on the quesion, is the 2nd sentece logicaly continue the 1st sentance\n",
    "#print('Sentence is okey: ', not bool(np.argmax(predicts, axis=-1)[0]), predicts)\n",
    "#print('Sentence is okey:', int(round(predicts[0][0]*100)), '%')                    # [[0.9657724  0.03422766]] - left number is probability that  2nd senetence logicaly continue 1st sentence, right number - probability that it's random\n",
    "print('Sentences is matched on:', round(predicts[0][0]*100,2), '%')\n",
    "out = int(round(predicts[0][0]*100)) \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
